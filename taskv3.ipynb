{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "#data (as pandas dataframes) \n",
    "train_data = adult.data.features \n",
    "test_data = adult.data.targets \n",
    "\n",
    "#rename income classes to be 2 classes only\n",
    "test_data['income'] = [i.replace('.','') for i in test_data['income']]\n",
    "\n",
    "#metadata \n",
    "print(adult.metadata) \n",
    "  \n",
    "#variable information \n",
    "print(adult.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, test_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check missing values in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values for training data:  1758\n",
      "Percentage of missing values for training data:\n",
      "age               0.000000\n",
      "workclass         1.975789\n",
      "fnlwgt            0.000000\n",
      "education         0.000000\n",
      "education-num     0.000000\n",
      "marital-status    0.000000\n",
      "occupation        1.980908\n",
      "relationship      0.000000\n",
      "race              0.000000\n",
      "sex               0.000000\n",
      "capital-gain      0.000000\n",
      "capital-loss      0.000000\n",
      "hours-per-week    0.000000\n",
      "native-country    0.542574\n",
      "dtype: float64\n",
      "The column with the highest missing values percentage for training data is: occupation\n",
      "============================================================\n",
      "Total number of missing values for testing data:  445\n",
      "Percentage of missing values for testing data:\n",
      "age               0.000000\n",
      "workclass         1.955164\n",
      "fnlwgt            0.000000\n",
      "education         0.000000\n",
      "education-num     0.000000\n",
      "marital-status    0.000000\n",
      "occupation        1.965401\n",
      "relationship      0.000000\n",
      "race              0.000000\n",
      "sex               0.000000\n",
      "capital-gain      0.000000\n",
      "capital-loss      0.000000\n",
      "hours-per-week    0.000000\n",
      "native-country    0.634661\n",
      "dtype: float64\n",
      "The column with the highest missing values percentage for testing data is: occupation\n"
     ]
    }
   ],
   "source": [
    "missing_values_train = X_train.isnull().values.sum()\n",
    "print(\"Total number of missing values for training data: \",missing_values_train)\n",
    "\n",
    "#percentage of null values\n",
    "missing_percentage_train = (X_train.isnull().sum() / len(X_train)) * 100\n",
    "print(\"Percentage of missing values for training data:\")\n",
    "print(missing_percentage_train)\n",
    "\n",
    "#highest column percentage of null values\n",
    "highest_missing_percentage_train = missing_percentage_train.idxmax()\n",
    "print(f\"The column with the highest missing values percentage for training data is: {highest_missing_percentage_train}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "missing_values_test = X_test.isnull().values.sum()\n",
    "print(\"Total number of missing values for testing data: \",missing_values_test)\n",
    "\n",
    "#percentage of null values\n",
    "missing_percentage_test = (X_test.isnull().sum() / len(X_test)) * 100\n",
    "print(\"Percentage of missing values for testing data:\")\n",
    "print(missing_percentage_test)\n",
    "\n",
    "#highest column percentage of null values\n",
    "highest_missing_percentage_test = missing_percentage_test.idxmax()\n",
    "print(f\"The column with the highest missing values percentage for testing data is: {highest_missing_percentage_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing data in categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_10012\\1342074074.py:4: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in X_train.columns if pd.api.types.is_categorical_dtype(X_train[col]) or X_train[col].dtype == 'object']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values for training data:  0\n",
      "Percentage of missing values for training data:\n",
      "age               0.0\n",
      "workclass         0.0\n",
      "fnlwgt            0.0\n",
      "education         0.0\n",
      "education-num     0.0\n",
      "marital-status    0.0\n",
      "occupation        0.0\n",
      "relationship      0.0\n",
      "race              0.0\n",
      "sex               0.0\n",
      "capital-gain      0.0\n",
      "capital-loss      0.0\n",
      "hours-per-week    0.0\n",
      "native-country    0.0\n",
      "dtype: float64\n",
      "============================================================\n",
      "Total number of missing values for testing data:  0\n",
      "Percentage of missing values for testing data:\n",
      "age               0.0\n",
      "workclass         0.0\n",
      "fnlwgt            0.0\n",
      "education         0.0\n",
      "education-num     0.0\n",
      "marital-status    0.0\n",
      "occupation        0.0\n",
      "relationship      0.0\n",
      "race              0.0\n",
      "sex               0.0\n",
      "capital-gain      0.0\n",
      "capital-loss      0.0\n",
      "hours-per-week    0.0\n",
      "native-country    0.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_10012\\1342074074.py:21: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  categorical_cols = [col for col in X_test.columns if pd.api.types.is_categorical_dtype(X_test[col]) or X_test[col].dtype == 'object']\n"
     ]
    }
   ],
   "source": [
    "numImpute = SimpleImputer(strategy='mean')\n",
    "catImpute = SimpleImputer(strategy='most_frequent')\n",
    "numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = [col for col in X_train.columns if pd.api.types.is_categorical_dtype(X_train[col]) or X_train[col].dtype == 'object']\n",
    "X_train[numerical_columns] = numImpute.fit_transform(X_train[numerical_columns])\n",
    "X_train[categorical_cols] = catImpute.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "missing_values_train = X_train.isnull().values.sum()\n",
    "print(\"Total number of missing values for training data: \",missing_values_train)\n",
    "\n",
    "#percentage of null values\n",
    "missing_percentage_train = (X_train.isnull().sum() / len(X_train)) * 100\n",
    "print(\"Percentage of missing values for training data:\")\n",
    "print(missing_percentage_train)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "numImpute = SimpleImputer(strategy='mean')\n",
    "catImpute = SimpleImputer(strategy='most_frequent')\n",
    "numerical_columns = X_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = [col for col in X_test.columns if pd.api.types.is_categorical_dtype(X_test[col]) or X_test[col].dtype == 'object']\n",
    "X_test[numerical_columns] = numImpute.fit_transform(X_test[numerical_columns])\n",
    "X_test[categorical_cols] = catImpute.fit_transform(X_test[categorical_cols])\n",
    "\n",
    "missing_values_test = X_test.isnull().values.sum()\n",
    "print(\"Total number of missing values for testing data: \",missing_values_test)\n",
    "\n",
    "#percentage of null values\n",
    "missing_percentage_test = (X_test.isnull().sum() / len(X_test)) * 100\n",
    "print(\"Percentage of missing values for testing data:\")\n",
    "print(missing_percentage_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=categorical_cols)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols)\n",
    "\n",
    "#ensure the train and test datasets have the same columns after encoding\n",
    "missing_cols_test = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols_test:\n",
    "    X_test[col] = 0\n",
    "\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Naive Bayes classifier and make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values for training data:  0\n",
      "Percentage of missing values for training data:\n",
      "age                               0.0\n",
      "fnlwgt                            0.0\n",
      "education-num                     0.0\n",
      "capital-gain                      0.0\n",
      "capital-loss                      0.0\n",
      "                                 ... \n",
      "native-country_Thailand           0.0\n",
      "native-country_Trinadad&Tobago    0.0\n",
      "native-country_United-States      0.0\n",
      "native-country_Vietnam            0.0\n",
      "native-country_Yugoslavia         0.0\n",
      "Length: 107, dtype: float64\n",
      "Total number of missing values for testing data:  0\n",
      "Percentage of missing values for testing data:\n",
      "age                               0.0\n",
      "fnlwgt                            0.0\n",
      "education-num                     0.0\n",
      "capital-gain                      0.0\n",
      "capital-loss                      0.0\n",
      "                                 ... \n",
      "native-country_Thailand           0.0\n",
      "native-country_Trinadad&Tobago    0.0\n",
      "native-country_United-States      0.0\n",
      "native-country_Vietnam            0.0\n",
      "native-country_Yugoslavia         0.0\n",
      "Length: 107, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programs\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate statistics and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.81      0.95      0.88      7414\n",
      "        >50K       0.67      0.31      0.42      2355\n",
      "\n",
      "    accuracy                           0.80      9769\n",
      "   macro avg       0.74      0.63      0.65      9769\n",
      "weighted avg       0.78      0.80      0.77      9769\n",
      "\n",
      "Confusion Matrix Shape: (2, 2)\n",
      "Sensitivity: 0.30530785562632695, Specificity: 0.9511734556244942\n",
      "Posterior probabilities of making over 50K a year:\n",
      "[0.0003394  0.0003338  0.00294792 ... 0.01748743 0.02171417 0.01308272]\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "#generate and print the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "#calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#check the shape of the confusion matrix\n",
    "print(\"Confusion Matrix Shape:\", conf_matrix.shape)\n",
    "\n",
    "#extract values from the confusion matrix\n",
    "if conf_matrix.shape == (2, 2):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    #compute Sensitivity and Specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(f\"Sensitivity: {sensitivity}, Specificity: {specificity}\")\n",
    "else:\n",
    "    print(\"Error: Confusion matrix is not of shape (2, 2), check your data and predictions.\")\n",
    "\n",
    "#compute the posterior probability of making over 50K a year\n",
    "posterior_probabilities = nb_classifier.predict_proba(X_test)[:, 1]\n",
    "print(\"Posterior probabilities of making over 50K a year:\")\n",
    "print(posterior_probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
